{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68427fd",
   "metadata": {},
   "source": [
    "### Setup and evaluation helpers\n",
    "- Imports, environment creators, and random seed\n",
    "- `reward_scalar` to safely convert reward arrays to Python floats\n",
    "- `epsilon_greedy_action`, `is_terminal`, and `evaluate_policy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL: SARSA and Q-Learning on Grid World\n",
    "# Uses env.create_standard_grid and env.create_four_room\n",
    "\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from env import create_standard_grid, create_four_room\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Safe reward scalar extractor for 1-element arrays\n",
    "def reward_scalar(r):\n",
    "    return float(np.ravel(r)[0])\n",
    "\n",
    "# epsilon greedy function\n",
    "# Chose to use correct datatype outputs as it becomes a problem in downstream tasks.\n",
    "\n",
    "def epsilon_greedy_action(Q: np.ndarray, state: int, epsilon: float) -> int:\n",
    "    \"\"\"Pick epsilon-greedy action from Q[state].\"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    return int(np.argmax(Q[state]))\n",
    "\n",
    "# --- Episode termination check ---\n",
    "def is_terminal(state: int, goal_states_seq: np.ndarray) -> bool:\n",
    "    return state in set(map(int, np.array(goal_states_seq).flatten()))\n",
    "\n",
    "# --- Rollouts for evaluation ---\n",
    "def evaluate_policy(env, Q: np.ndarray, episodes: int = 20, max_steps: int = 100) -> float:\n",
    "    total = 0.0\n",
    "    for _ in range(episodes):\n",
    "        s = env.reset()\n",
    "        ep_ret = 0.0\n",
    "        for _ in range(max_steps):\n",
    "            a = int(np.argmax(Q[s]))\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "            s = int(s_next)\n",
    "            if is_terminal(s, env.goal_states_seq):\n",
    "                break\n",
    "        total += ep_ret\n",
    "    return total / episodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5eceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginner-friendly helpers (no type hints)\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy_action(Q, state, epsilon):\n",
    "    if np.random.rand() < float(epsilon):\n",
    "        return int(np.random.randint(Q.shape[1]))\n",
    "    return int(np.argmax(Q[state]))\n",
    "\n",
    "\n",
    "def is_terminal(state, goal_states_seq):\n",
    "    # goal_states_seq may be nested arrays; flatten and compare as ints\n",
    "    return int(state) in set(map(int, np.array(goal_states_seq).flatten()))\n",
    "\n",
    "\n",
    "def evaluate_policy(env, Q, episodes=20, max_steps=100):\n",
    "    total = 0.0\n",
    "    for _ in range(int(episodes)):\n",
    "        s = int(env.reset())\n",
    "        ep_ret = 0.0\n",
    "        for _ in range(int(max_steps)):\n",
    "            a = int(np.argmax(Q[s]))\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "            s = int(s_next)\n",
    "            if is_terminal(s, env.goal_states_seq):\n",
    "                break\n",
    "        total += ep_ret\n",
    "    return total / float(episodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginner-friendly training loops (SARSA and Q-Learning)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_sarsa(env, episodes=2000, alpha=0.1, gamma=0.99,\n",
    "                epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_episodes=1500,\n",
    "                max_steps=100):\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    Q = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def eps_schedule(ep):\n",
    "        if epsilon_decay_episodes <= 0:\n",
    "            return float(epsilon_end)\n",
    "        frac = min(1.0, float(ep) / float(epsilon_decay_episodes))\n",
    "        return float(epsilon_start) + frac * (float(epsilon_end) - float(epsilon_start))\n",
    "\n",
    "    returns = []\n",
    "    for ep in range(int(episodes)):\n",
    "        s = int(env.reset())\n",
    "        eps = eps_schedule(ep)\n",
    "        a = epsilon_greedy_action(Q, s, eps)\n",
    "        ep_ret = 0.0\n",
    "        for _ in range(int(max_steps)):\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "            s_next = int(s_next)\n",
    "\n",
    "            if is_terminal(s_next, env.goal_states_seq):\n",
    "                td_target = reward_scalar(r)\n",
    "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "                break\n",
    "\n",
    "            a_next = epsilon_greedy_action(Q, s_next, eps)\n",
    "            td_target = reward_scalar(r) + gamma * Q[s_next, a_next]\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "            s, a = s_next, a_next\n",
    "        returns.append(ep_ret)\n",
    "    return Q, returns\n",
    "\n",
    "\n",
    "def train_q_learning(env, episodes=2000, alpha=0.1, gamma=0.99,\n",
    "                      epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_episodes=1500,\n",
    "                      max_steps=100):\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    Q = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def eps_schedule(ep):\n",
    "        if epsilon_decay_episodes <= 0:\n",
    "            return float(epsilon_end)\n",
    "        frac = min(1.0, float(ep) / float(epsilon_decay_episodes))\n",
    "        return float(epsilon_start) + frac * (float(epsilon_end) - float(epsilon_start))\n",
    "\n",
    "    returns = []\n",
    "    for ep in range(int(episodes)):\n",
    "        s = int(env.reset())\n",
    "        eps = eps_schedule(ep)\n",
    "        ep_ret = 0.0\n",
    "        for _ in range(int(max_steps)):\n",
    "            a = epsilon_greedy_action(Q, s, eps)\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "            s_next = int(s_next)\n",
    "\n",
    "            if is_terminal(s_next, env.goal_states_seq):\n",
    "                td_target = reward_scalar(r)\n",
    "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "                break\n",
    "\n",
    "            best_next = float(np.max(Q[s_next]))\n",
    "            td_target = reward_scalar(r) + gamma * best_next\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "            s = s_next\n",
    "        returns.append(ep_ret)\n",
    "    return Q, returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax_action(Q, state, tau):\n",
    "    tau = float(tau)\n",
    "    if tau <= 0:\n",
    "        return int(np.argmax(Q[state]))\n",
    "    z = Q[state] / tau\n",
    "    z = z - np.max(z)\n",
    "    p = np.exp(z)\n",
    "    p = p / np.sum(p)\n",
    "    return int(np.random.choice(len(p), p=p))\n",
    "\n",
    "\n",
    "def select_action(Q, state, strategy, param):\n",
    "    if strategy == 'eps_greedy':\n",
    "        if np.random.rand() < float(param):\n",
    "            return int(np.random.randint(Q.shape[1]))\n",
    "        return int(np.argmax(Q[state]))\n",
    "    elif strategy == 'softmax':\n",
    "        return softmax_action(Q, state, float(param))\n",
    "    else:\n",
    "        raise ValueError('Unknown exploration strategy: ' + str(strategy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa_fixed(env, episodes, alpha, gamma, strategy, param, max_steps=200,\n",
    "                      early_stop=True, early_stop_window=200, early_stop_tol=0.05):\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    Q = np.zeros((num_states, num_actions), dtype=float)\n",
    "    returns = []\n",
    "\n",
    "    for _ in range(int(episodes)):\n",
    "        s = int(env.reset())\n",
    "        a = select_action(Q, s, strategy, param)\n",
    "        ep_ret = 0.0\n",
    "        for _ in range(int(max_steps)):\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "            s_next = int(s_next)\n",
    "\n",
    "            if is_terminal(s_next, env.goal_states_seq):\n",
    "                td_target = reward_scalar(r)\n",
    "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "                break\n",
    "\n",
    "            a_next = select_action(Q, s_next, strategy, param)\n",
    "            td_target = reward_scalar(r) + gamma * Q[s_next, a_next]\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "            s, a = s_next, a_next\n",
    "        returns.append(ep_ret)\n",
    "    return Q, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_q_learning_fixed(env, episodes, alpha, gamma, strategy, param, max_steps=200):\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    Q = np.zeros((num_states, num_actions), dtype=float)\n",
    "    returns = []\n",
    "\n",
    "    for _ in range(int(episodes)):\n",
    "        s = int(env.reset())\n",
    "        ep_ret = 0.0\n",
    "        for _ in range(int(max_steps)):\n",
    "            a = select_action(Q, s, strategy, param)\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "            s_next = int(s_next)\n",
    "\n",
    "            if is_terminal(s_next, env.goal_states_seq):\n",
    "                td_target = reward_scalar(r)\n",
    "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "                break\n",
    "\n",
    "            best_next = float(np.max(Q[s_next]))\n",
    "            td_target = reward_scalar(r) + gamma * best_next\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "            s = s_next\n",
    "        returns.append(ep_ret)\n",
    "    return Q, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4f105",
   "metadata": {},
   "source": [
    "### SARSA (epsilon-decay baseline)\n",
    "- Reference implementation of SARSA with epsilon decay\n",
    "- Used for quick demos; sweeps use fixed-parameter trainer instead\n",
    "- Returns Q-table and per-episode returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa(\n",
    "    env,\n",
    "    episodes: int = 2000,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon_start: float = 1.0,\n",
    "    epsilon_end: float = 0.05,\n",
    "    epsilon_decay_episodes: int = 1500,\n",
    "    max_steps: int = 100,\n",
    "):\n",
    "\n",
    "    \"\"\"On-policy TD control (SARSA).\"\"\"\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    Q = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def eps_schedule(ep):\n",
    "        if epsilon_decay_episodes <= 0:\n",
    "            return epsilon_end\n",
    "        frac = min(1.0, ep / epsilon_decay_episodes)\n",
    "        return epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "\n",
    "    returns: List[float] = []\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        eps = eps_schedule(ep)\n",
    "        a = epsilon_greedy_action(Q, s, eps)\n",
    "        ep_ret = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "\n",
    "            if is_terminal(int(s_next), env.goal_states_seq):\n",
    "                td_target = reward_scalar(r)\n",
    "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "                break\n",
    "\n",
    "            a_next = epsilon_greedy_action(Q, int(s_next), eps)\n",
    "            td_target = reward_scalar(r) + gamma * Q[int(s_next), a_next]\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "\n",
    "            s, a = int(s_next), a_next\n",
    "\n",
    "        returns.append(ep_ret)\n",
    "\n",
    "    return Q, returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c12c73",
   "metadata": {},
   "source": [
    "### Q-learning (epsilon-decay baseline)\n",
    "- Reference implementation of Q-learning with epsilon decay\n",
    "- Used for quick demos; sweeps use fixed-parameter trainer instead\n",
    "- Returns Q-table and per-episode returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(\n",
    "    env,\n",
    "    episodes: int = 2000,\n",
    "    alpha: float = 0.1,\n",
    "    gamma: float = 0.99,\n",
    "    epsilon_start: float = 1.0,\n",
    "    epsilon_end: float = 0.05,\n",
    "    epsilon_decay_episodes: int = 1500,\n",
    "    max_steps: int = 100,\n",
    "):\n",
    "    \"\"\"Off-policy TD control (Q-Learning).\"\"\"\n",
    "    num_states, num_actions = env.num_states, env.num_actions\n",
    "    Q = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def eps_schedule(ep):\n",
    "        if epsilon_decay_episodes <= 0:\n",
    "            return epsilon_end\n",
    "        frac = min(1.0, ep / epsilon_decay_episodes)\n",
    "        return epsilon_start + frac * (epsilon_end - epsilon_start)\n",
    "\n",
    "    returns: List[float] = []\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        eps = eps_schedule(ep)\n",
    "        ep_ret = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            a = epsilon_greedy_action(Q, s, eps)\n",
    "            s_next, r = env.step(s, a)\n",
    "            ep_ret += reward_scalar(r)\n",
    "\n",
    "            if is_terminal(int(s_next), env.goal_states_seq):\n",
    "                td_target = reward_scalar(r)\n",
    "                Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "                break\n",
    "\n",
    "            best_next = np.max(Q[int(s_next)])\n",
    "            td_target = reward_scalar(r) + gamma * best_next\n",
    "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
    "\n",
    "            s = int(s_next)\n",
    "\n",
    "        returns.append(ep_ret)\n",
    "\n",
    "    return Q, returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77b51b4",
   "metadata": {},
   "source": [
    "### 10×10 quick training demo\n",
    "- Demonstration runs of Q-learning and SARSA on the 10×10 grid\n",
    "- Uses simple epsilon decay (older helpers) to sanity-check training\n",
    "- Prints average evaluation return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run: 10x10 Standard Grid (stochastic and wind optional) ---\n",
    "std_env = create_standard_grid(start_state=np.array([[0,4]]), transition_prob=0.7, wind=False)\n",
    "\n",
    "# Q-Learning on standard grid\n",
    "Q_q, ret_q = train_q_learning(std_env, episodes=3000, alpha=0.1, gamma=0.99,\n",
    "                              epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_episodes=2500)\n",
    "print(\"Standard Grid — Q-Learning eval avg return:\", evaluate_policy(std_env, Q_q))\n",
    "\n",
    "# SARSA on standard grid\n",
    "Q_s, ret_s = train_sarsa(std_env, episodes=3000, alpha=0.1, gamma=0.99,\n",
    "                         epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_episodes=2500)\n",
    "print(\"Standard Grid — SARSA eval avg return:\", evaluate_policy(std_env, Q_s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "714423bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saveable learning curve plots and sweep plotting flags\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLOT_SWEEP = True  # set False to skip saving plots during sweeps\n",
    "PLOT_DIR = 'sweep_plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_learning_curves(returns_list, labels, window=100, title='Episode returns', filepath=None):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for returns, label in zip(returns_list, labels):\n",
    "        plt.plot(returns, alpha=0.3, label=str(label) + ' (raw)')\n",
    "        # simple rolling mean (same as earlier helper)\n",
    "        x = np.array(returns, dtype=float)\n",
    "        w = min(int(window), len(x)) if len(x) > 0 else 1\n",
    "        csum = np.cumsum(np.insert(x, 0, 0.0))\n",
    "        rm = (csum[w:] - csum[:-w]) / float(w)\n",
    "        pad = np.full(w-1, np.nan)\n",
    "        y = np.concatenate([pad, rm]) if len(x) > 0 else x\n",
    "        plt.plot(y, linewidth=2, label=str(label) + f' (rolling {window})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Return')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if filepath:\n",
    "        plt.savefig(filepath, bbox_inches='tight', dpi=120)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178a703",
   "metadata": {},
   "source": [
    "### Four-Room quick training demo\n",
    "- Demonstration runs of Q-learning and SARSA on the Four-Room environment\n",
    "- Uses simple epsilon decay (older helpers) to sanity-check training\n",
    "- Prints average evaluation return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run: Four-Room Grid (deterministic, dynamic goal) ---\n",
    "fr_env = create_four_room(goal_change=True, transition_prob=1.0)\n",
    "\n",
    "Q_q_fr, ret_q_fr = train_q_learning(fr_env, episodes=3000, alpha=0.1, gamma=0.99,\n",
    "                                    epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_episodes=2500)\n",
    "print(\"Four-Room — Q-Learning eval avg return:\", evaluate_policy(fr_env, Q_q_fr))\n",
    "\n",
    "Q_s_fr, ret_s_fr = train_sarsa(fr_env, episodes=3000, alpha=0.1, gamma=0.99,\n",
    "                               epsilon_start=1.0, epsilon_end=0.05, epsilon_decay_episodes=2500)\n",
    "print(\"Four-Room — SARSA eval avg return:\", evaluate_policy(fr_env, Q_s_fr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b253aa1",
   "metadata": {},
   "source": [
    "### Plotting utilities and convergence heuristic\n",
    "- Rolling mean computation and learning curve plotting\n",
    "- Simple convergence check based on stability of rolling mean tail\n",
    "- Example plots for prior quick runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rolling_mean(x, window=100):\n",
    "    if len(x) == 0:\n",
    "        return []\n",
    "    w = min(window, len(x))\n",
    "    cumsum = np.cumsum(np.insert(np.array(x, dtype=float), 0, 0.0))\n",
    "    rm = (cumsum[w:] - cumsum[:-w]) / w\n",
    "    # pad to match length\n",
    "    pad = np.full(w-1, np.nan)\n",
    "    return np.concatenate([pad, rm])\n",
    "\n",
    "def plot_learning_curves(returns_list, labels, window=100, title=\"Episode returns\"):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for returns, label in zip(returns_list, labels):\n",
    "        plt.plot(returns, alpha=0.3, label=f\"{label} (raw)\")\n",
    "        plt.plot(rolling_mean(returns, window), linewidth=2, label=f\"{label} (rolling {window})\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Heuristic: convergence when rolling mean change is small over last K episodes\n",
    "def has_converged(reward_list, window=100, min_avg=-150, std_threshold=10):\n",
    "    if len(reward_list) < window:\n",
    "        return False\n",
    "    window_rewards = reward_list[-window:]\n",
    "    mean = np.mean(window_rewards)\n",
    "    std = np.std(window_rewards)\n",
    "    # Converged if mean reward > target and variance is low\n",
    "    return (mean > min_avg) and (std < std_threshold)\n",
    "\n",
    "# If you already ran training above, you can visualize now:\n",
    "try:\n",
    "    plot_learning_curves([ret_q, ret_s], [\"Q-Learning (standard)\", \"SARSA (standard)\"], window=100,\n",
    "                         title=\"Standard Grid: Episode Returns\")\n",
    "    print(\"Standard Q-Learning converged?\", has_converged(ret_q, window=200))\n",
    "    print(\"Standard SARSA converged?\", has_converged(ret_s, window=200))\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    plot_learning_curves([ret_q_fr, ret_s_fr], [\"Q-Learning (four-room)\", \"SARSA (four-room)\"], window=100,\n",
    "                         title=\"Four-Room: Episode Returns\")\n",
    "    print(\"Four-Room Q-Learning converged?\", has_converged(ret_q_fr, window=200))\n",
    "    print(\"Four-Room SARSA converged?\", has_converged(ret_s_fr, window=200))\n",
    "except NameError:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d8c04a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Experiment configuration\n",
    "# -----------------------\n",
    "\n",
    "EPISODES = 3000          # total training episodes per run\n",
    "MAX_STEPS = 100          # max steps per episode\n",
    "EVAL_EPISODES = 100      # for post-training evaluation\n",
    "SEEDS = [42, 100, 202, 303, 404]\n",
    "\n",
    "# Hyperparameter ranges (from assignment)\n",
    "ALPHAS = [0.001, 0.01, 0.1, 1.0]\n",
    "GAMMAS = [0.7, 0.8, 0.9, 1.0]\n",
    "EPSILONS = [0.001, 0.01, 0.05, 0.1]\n",
    "TAUS = [0.01, 0.1, 1.0, 2.0]\n",
    "\n",
    "# Convergence and plotting params\n",
    "WINDOW = 100\n",
    "THRESHOLD = 5.0\n",
    "RESULTS_DIR = \"./results\"\n",
    "\n",
    "# Ensure result folder exists\n",
    "import os\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370dd6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from env import create_standard_grid, create_four_room\n",
    "\n",
    "all_configs = []\n",
    "\n",
    "# -------------------------------\n",
    "# 10x10 Grid World: Q-learning (8)\n",
    "# -------------------------------\n",
    "for tp in [0.7, 1.0]:\n",
    "    for start_state in [(0, 4), (3, 6)]:\n",
    "        for strat in ['eps_greedy', 'softmax']:\n",
    "            name = f\"std_q_tp{tp}_ss{start_state}_strat{strat}\"\n",
    "            cfg = {\n",
    "                'name': name,\n",
    "                'alg': 'q_learning',\n",
    "                'env_builder': create_standard_grid,\n",
    "                'env_kwargs': {\n",
    "                    # ✅ FIX: convert tuple to 2D numpy array\n",
    "                    'start_state': np.array([[start_state[0], start_state[1]]]),\n",
    "                    'transition_prob': tp,\n",
    "                    'wind': False\n",
    "                },\n",
    "                'strategy': strat\n",
    "            }\n",
    "            all_configs.append(cfg)\n",
    "\n",
    "# -------------------------------\n",
    "# 10x10 Grid World: SARSA (8)\n",
    "# -------------------------------\n",
    "for wind in [True, False]:\n",
    "    for start_state in [(0, 4), (3, 6)]:\n",
    "        for strat in ['eps_greedy', 'softmax']:\n",
    "            name = f\"std_sarsa_wind{wind}_ss{start_state}_strat{strat}\"\n",
    "            cfg = {\n",
    "                'name': name,\n",
    "                'alg': 'sarsa',\n",
    "                'env_builder': create_standard_grid,\n",
    "                'env_kwargs': {\n",
    "                    # ✅ FIX here too\n",
    "                    'start_state': np.array([[start_state[0], start_state[1]]]),\n",
    "                    'transition_prob': 1.0,\n",
    "                    'wind': wind\n",
    "                },\n",
    "                'strategy': strat\n",
    "            }\n",
    "            all_configs.append(cfg)\n",
    "\n",
    "# -------------------------------\n",
    "# Four-Room configs (no fix needed)\n",
    "# -------------------------------\n",
    "for goal_change in [True, False]:\n",
    "    all_configs.append({\n",
    "        'name': f\"four_q_goalchange{goal_change}_strateps_greedy\",\n",
    "        'alg': 'q_learning',\n",
    "        'env_builder': create_four_room,\n",
    "        'env_kwargs': {'goal_change': goal_change},\n",
    "        'strategy': 'eps_greedy'\n",
    "    })\n",
    "for goal_change in [True, False]:\n",
    "    all_configs.append({\n",
    "        'name': f\"four_sarsa_goalchange{goal_change}_strateps_greedy\",\n",
    "        'alg': 'sarsa',\n",
    "        'env_builder': create_four_room,\n",
    "        'env_kwargs': {'goal_change': goal_change},\n",
    "        'strategy': 'eps_greedy'\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "248b9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_convergence_curve(reward_list, window=100, title=\"Convergence Plot\", config_name=None):\n",
    "    \"\"\"\n",
    "    Plot rewards and their rolling average to visually inspect convergence.\n",
    "    \n",
    "    Parameters:\n",
    "    - reward_list: list or array of episode rewards\n",
    "    - window: size of smoothing window for moving average\n",
    "    - title: custom plot title\n",
    "    - config_name: optional configuration name to display on plot\n",
    "    \"\"\"\n",
    "    rewards = np.array(reward_list)\n",
    "    episodes = np.arange(len(rewards)) + 1\n",
    "\n",
    "    # Rolling average\n",
    "    rolling = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(episodes, rewards, color='lightgray', alpha=0.6, label='Raw Episode Reward')\n",
    "    plt.plot(episodes[window-1:], rolling, color='blue', linewidth=2.0, label=f'{window}-Episode Moving Average')\n",
    "\n",
    "    # Annotate convergence visually\n",
    "    plt.axvline(len(rewards) - window, color='red', linestyle='--', alpha=0.4)\n",
    "    plt.text(len(rewards) - window + 10, np.mean(rolling[-50:]),\n",
    "             'Recent avg\\n(Converged region?)', color='red', fontsize=8)\n",
    "\n",
    "    plt.xlabel('Episode', fontsize=11)\n",
    "    plt.ylabel('Reward per Episode', fontsize=11)\n",
    "    if config_name:\n",
    "        plt.title(f\"{title}\\n({config_name})\", fontsize=12)\n",
    "    else:\n",
    "        plt.title(title, fontsize=12)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2243a5",
   "metadata": {},
   "source": [
    "### Exploration helpers\n",
    "- Epsilon-greedy and softmax action selection\n",
    "- Numerically-stable softmax with temperature τ\n",
    "- Unified `select_action` dispatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56529e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beginner-friendly sweep helpers with convergence tracking and optional plots\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Overwrite lightweight runners to also return returns for convergence checks\n",
    "\n",
    "def run_one_training(alg, env, alpha, gamma, strategy, param):\n",
    "    if alg == 'q_learning':\n",
    "        Q, returns = train_q_learning_fixed(env, episodes=EPISODES, alpha=alpha, gamma=gamma,\n",
    "                                            strategy=strategy, param=param, max_steps=MAX_STEPS)\n",
    "    elif alg == 'sarsa':\n",
    "        Q, returns = train_sarsa_fixed(env, episodes=EPISODES, alpha=alpha, gamma=gamma,\n",
    "                                       strategy=strategy, param=param, max_steps=MAX_STEPS)\n",
    "    else:\n",
    "        raise ValueError(\"alg must be 'q_learning' or 'sarsa'\")\n",
    "    avg_eval = float(evaluate_policy(env, Q, episodes=EVAL_EPISODES, max_steps=MAX_STEPS))\n",
    "    return avg_eval, returns\n",
    "\n",
    "\n",
    "def sweep_hparams_for_config(alg, env_builder, env_kwargs, strategy):\n",
    "    records = []\n",
    "    explore_params = EPSILONS if strategy == 'eps_greedy' else TAUS\n",
    "\n",
    "    for alpha, gamma, param in itertools.product(ALPHAS, GAMMAS, explore_params):\n",
    "        evals = []\n",
    "        converged_flags = []\n",
    "        drifts = []\n",
    "        per_seed_returns = []\n",
    "        for seed in SEEDS:\n",
    "            np.random.seed(seed)\n",
    "            env = env_builder(**env_kwargs)\n",
    "            avg_eval, returns = run_one_training(alg, env, alpha, gamma, strategy, param)\n",
    "            evals.append(avg_eval)\n",
    "            # Skip numerical convergence check — using visual plots instead\n",
    "            cflag, drift = False, np.nan\n",
    "            converged_flags.append(cflag)\n",
    "            drifts.append(drift)\n",
    "            per_seed_returns.append(returns)\n",
    "\n",
    "\n",
    "        mean_eval = float(np.mean(evals))\n",
    "        std_eval = float(np.std(evals))\n",
    "        frac_converged = float(np.mean(converged_flags))\n",
    "        \n",
    "\n",
    "        # Optional: save a plot for this hyperparameter triple averaged across seeds\n",
    "        if PLOT_SWEEP:\n",
    "            title = f\"{alg} | {strategy} param={param} | alpha={alpha}, gamma={gamma}\"\n",
    "            fname = f\"{alg}_{strategy}_a{alpha}_g{gamma}_p{param}.png\".replace(' ', '')\n",
    "            path = os.path.join(PLOT_DIR, fname)\n",
    "            labels = [f\"seed {s}\" for s in SEEDS]\n",
    "            save_learning_curves(per_seed_returns, labels, window=100, title=title, filepath=path)\n",
    "\n",
    "        rec = {\n",
    "            'alg': alg,\n",
    "            'strategy': strategy,\n",
    "            'alpha': alpha,\n",
    "            'gamma': gamma,\n",
    "            'param': param,\n",
    "            'mean_eval_return': mean_eval,\n",
    "            'std_eval_return': std_eval,\n",
    "            **{f'cfg_{k}': v for k, v in env_kwargs.items()},\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    best_row = df.loc[df['mean_eval_return'].idxmax()]\n",
    "    return df, dict(best_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c1e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to run exactly one configuration by name\n",
    "import pandas as pd\n",
    "\n",
    "def run_one_config_by_name(config_name, episodes=None, eval_episodes=None, save_prefix=None):\n",
    "    # local override of episode counts if provided\n",
    "    global EPISODES, EVAL_EPISODES\n",
    "    old_EP, old_EVAL = EPISODES, EVAL_EPISODES\n",
    "    if episodes is not None:\n",
    "        EPISODES = int(episodes)\n",
    "    if eval_episodes is not None:\n",
    "        EVAL_EPISODES = int(eval_episodes)\n",
    "\n",
    "    cfg = None\n",
    "    for c in all_configs:\n",
    "        if c['name'] == config_name:\n",
    "            cfg = c\n",
    "            break\n",
    "    if cfg is None:\n",
    "        raise ValueError('Unknown config name: ' + str(config_name))\n",
    "\n",
    "    print('Running single config:', cfg['name'])\n",
    "    df, best = sweep_hparams_for_config(\n",
    "        alg=cfg['alg'],\n",
    "        env_builder=cfg['env_builder'],\n",
    "        env_kwargs=cfg['env_kwargs'],\n",
    "        strategy=cfg['strategy'],\n",
    "    )\n",
    "    df['config_name'] = cfg['name']\n",
    "    best['config_name'] = cfg['name']\n",
    "\n",
    "    # Save CSVs using a readable prefix\n",
    "    prefix = save_prefix if save_prefix else cfg['name']\n",
    "    results_path = f'{prefix}_results.csv'\n",
    "    best_path = f'{prefix}_best.csv'\n",
    "    df.to_csv(results_path, index=False)\n",
    "    pd.DataFrame([best]).to_csv(best_path, index=False)\n",
    "    print('Saved:', results_path, best_path)\n",
    "\n",
    "    # Restore globals\n",
    "    EPISODES, EVAL_EPISODES = old_EP, old_EVAL\n",
    "    return df, best\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf78b5d",
   "metadata": {},
   "source": [
    "### Run exactly one configuration per cell\n",
    "- Use the cells below; each runs one config end-to-end (sweep + save + plots).\n",
    "- You can tweak `episodes` and `eval_episodes` when calling `run_one_config_by_name`.\n",
    "- Files are saved as `<config-name>_results.csv` and `<config-name>_best.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in all_configs:\n",
    "    print(c['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f75eab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Config 1: std_q_tp0.7_ss(0,4)_strateps_greedy\n",
      "Running single config: std_q_tp0.7_ss(0, 4)_strateps_greedy\n",
      "Saved: std_q_tp0.7_ss(0, 4)_strateps_greedy_results.csv std_q_tp0.7_ss(0, 4)_strateps_greedy_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 1.0, 'param': 0.1, 'mean_eval_return': -16.45, 'std_eval_return': 2.9959973297718405, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 0.7, 'cfg_wind': False, 'config_name': 'std_q_tp0.7_ss(0, 4)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 1 – Q-Learning | tp=0.7 | start=(0,4) | ε-greedy\n",
    "print(\" Config 1: std_q_tp0.7_ss(0,4)_strateps_greedy\")\n",
    "df1, best1 = run_one_config_by_name('std_q_tp0.7_ss(0, 4)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac15336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Config 2: std_q_tp0.7_ss(0,4)_stratsoftmax\n",
      "Running single config: std_q_tp0.7_ss(0, 4)_stratsoftmax\n",
      "Saved: std_q_tp0.7_ss(0, 4)_stratsoftmax_results.csv std_q_tp0.7_ss(0, 4)_stratsoftmax_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 1.0, 'param': 0.01, 'mean_eval_return': -15.26, 'std_eval_return': 0.8151073548925933, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 0.7, 'cfg_wind': False, 'config_name': 'std_q_tp0.7_ss(0, 4)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 2 – Q-Learning | tp=0.7 | start=(0,4) | Softmax\n",
    "print(\" Config 2: std_q_tp0.7_ss(0,4)_stratsoftmax\")\n",
    "df2, best2 = run_one_config_by_name('std_q_tp0.7_ss(0, 4)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36a694c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Config 3: std_q_tp0.7_ss(3,6)_strateps_greedy\n",
      "Running single config: std_q_tp0.7_ss(3, 6)_strateps_greedy\n",
      "Saved: std_q_tp0.7_ss(3, 6)_strateps_greedy_results.csv std_q_tp0.7_ss(3, 6)_strateps_greedy_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 0.9, 'param': 0.1, 'mean_eval_return': -17.52, 'std_eval_return': 0.6592419889539806, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 0.7, 'cfg_wind': False, 'config_name': 'std_q_tp0.7_ss(3, 6)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 3 – Q-Learning | tp=0.7 | start=(3,6) | ε-greedy\n",
    "print(\" Config 3: std_q_tp0.7_ss(3,6)_strateps_greedy\")\n",
    "df3, best3 = run_one_config_by_name('std_q_tp0.7_ss(3, 6)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1312881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Config 4: std_q_tp0.7_ss(3,6)_stratsoftmax \n",
      "Running single config: std_q_tp0.7_ss(3, 6)_stratsoftmax\n",
      "Saved: std_q_tp0.7_ss(3, 6)_stratsoftmax_results.csv std_q_tp0.7_ss(3, 6)_stratsoftmax_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 1.0, 'param': 0.1, 'mean_eval_return': -17.759999999999998, 'std_eval_return': 2.5925662961629348, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 0.7, 'cfg_wind': False, 'config_name': 'std_q_tp0.7_ss(3, 6)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 4 – Q-Learning | tp=0.7 | start=(3,6) | Softmax\n",
    "print(\" Config 4: std_q_tp0.7_ss(3,6)_stratsoftmax \")\n",
    "df4, best4 = run_one_config_by_name('std_q_tp0.7_ss(3, 6)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7923919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 5: std_q_tp1.0_ss(0,4)_strateps_greedy\n",
      "Running single config: std_q_tp1.0_ss(0, 4)_strateps_greedy\n",
      "Saved: std_q_tp1.0_ss(0, 4)_strateps_greedy_results.csv std_q_tp1.0_ss(0, 4)_strateps_greedy_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.001, 'mean_eval_return': -6.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_q_tp1.0_ss(0, 4)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 5 – Q-Learning | tp=1.0 | start=(0,4) | ε-greedy\n",
    "print(\"Config 5: std_q_tp1.0_ss(0,4)_strateps_greedy\")\n",
    "df5, best5 = run_one_config_by_name('std_q_tp1.0_ss(0, 4)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "410cdce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 6: std_q_tp1.0_ss(0,4)_stratsoftmax\n",
      "Running single config: std_q_tp1.0_ss(0, 4)_stratsoftmax\n",
      "Saved: std_q_tp1.0_ss(0, 4)_stratsoftmax_results.csv std_q_tp1.0_ss(0, 4)_stratsoftmax_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.01, 'mean_eval_return': -6.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_q_tp1.0_ss(0, 4)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 6 – Q-Learning | tp=1.0 | start=(0,4) | Softmax\n",
    "print(\"Config 6: std_q_tp1.0_ss(0,4)_stratsoftmax\")\n",
    "df6, best6 = run_one_config_by_name('std_q_tp1.0_ss(0, 4)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ef1de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 7: std_q_tp1.0_ss(3,6)_strateps_greedy\n",
      "Running single config: std_q_tp1.0_ss(3, 6)_strateps_greedy\n",
      "Saved: std_q_tp1.0_ss(3, 6)_strateps_greedy_results.csv std_q_tp1.0_ss(3, 6)_strateps_greedy_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.001, 'mean_eval_return': -1.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_q_tp1.0_ss(3, 6)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 7 – Q-Learning | tp=1.0 | start=(3,6) | ε-greedy\n",
    "print(\"Config 7: std_q_tp1.0_ss(3,6)_strateps_greedy\")\n",
    "df7, best7 = run_one_config_by_name('std_q_tp1.0_ss(3, 6)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c9baac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 8: std_q_tp1.0_ss(3,6)_stratsoftmax\n",
      "Running single config: std_q_tp1.0_ss(3, 6)_stratsoftmax\n",
      "Saved: std_q_tp1.0_ss(3, 6)_stratsoftmax_results.csv std_q_tp1.0_ss(3, 6)_stratsoftmax_best.csv\n",
      "{'alg': 'q_learning', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.01, 'mean_eval_return': -1.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_q_tp1.0_ss(3, 6)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 8 – Q-Learning | tp=1.0 | start=(3,6) | Softmax\n",
    "print(\"Config 8: std_q_tp1.0_ss(3,6)_stratsoftmax\")\n",
    "df8, best8 = run_one_config_by_name('std_q_tp1.0_ss(3, 6)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6afeb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 9: std_sarsa_windTrue_ss(0,4)_strateps_greedy\n",
      "Running single config: std_sarsa_windTrue_ss(0, 4)_strateps_greedy\n",
      "Saved: std_sarsa_windTrue_ss(0, 4)_strateps_greedy_results.csv std_sarsa_windTrue_ss(0, 4)_strateps_greedy_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 0.9, 'param': 0.1, 'mean_eval_return': -7.720000000000001, 'std_eval_return': 0.5921148537234988, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 1.0, 'cfg_wind': True, 'config_name': 'std_sarsa_windTrue_ss(0, 4)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 9 – SARSA | wind=True | start=(0,4) | ε-greedy\n",
    "print(\"Config 9: std_sarsa_windTrue_ss(0,4)_strateps_greedy\")\n",
    "df9, best9 = run_one_config_by_name('std_sarsa_windTrue_ss(0, 4)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83773a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 10: std_sarsa_windTrue_ss(0,4)_stratsoftmax\n",
      "Running single config: std_sarsa_windTrue_ss(0, 4)_stratsoftmax\n",
      "Saved: std_sarsa_windTrue_ss(0, 4)_stratsoftmax_results.csv std_sarsa_windTrue_ss(0, 4)_stratsoftmax_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 0.9, 'param': 0.01, 'mean_eval_return': -7.590000000000001, 'std_eval_return': 0.5902541825349481, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 1.0, 'cfg_wind': True, 'config_name': 'std_sarsa_windTrue_ss(0, 4)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 10 – SARSA | wind=True | start=(0,4) | Softmax\n",
    "print(\"Config 10: std_sarsa_windTrue_ss(0,4)_stratsoftmax\")\n",
    "df10, best10 = run_one_config_by_name('std_sarsa_windTrue_ss(0, 4)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fee741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 11: std_sarsa_windTrue_ss(3,6)_strateps_greedy\n",
      "Running single config: std_sarsa_windTrue_ss(3, 6)_strateps_greedy\n",
      "Saved: std_sarsa_windTrue_ss(3, 6)_strateps_greedy_results.csv std_sarsa_windTrue_ss(3, 6)_strateps_greedy_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 0.9, 'param': 0.01, 'mean_eval_return': -4.1, 'std_eval_return': 0.4037325847637268, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 1.0, 'cfg_wind': True, 'config_name': 'std_sarsa_windTrue_ss(3, 6)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 11 – SARSA | wind=True | start=(3,6) | ε-greedy\n",
    "print(\"Config 11: std_sarsa_windTrue_ss(3,6)_strateps_greedy\")\n",
    "df11, best11 = run_one_config_by_name('std_sarsa_windTrue_ss(3, 6)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42a65f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 12: std_sarsa_windTrue_ss(3,6)_stratsoftmax\n",
      "Running single config: std_sarsa_windTrue_ss(3, 6)_stratsoftmax\n",
      "Saved: std_sarsa_windTrue_ss(3, 6)_stratsoftmax_results.csv std_sarsa_windTrue_ss(3, 6)_stratsoftmax_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 1.0, 'param': 0.1, 'mean_eval_return': -3.6399999999999997, 'std_eval_return': 0.534228415567723, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 1.0, 'cfg_wind': True, 'config_name': 'std_sarsa_windTrue_ss(3, 6)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 12 – SARSA | wind=True | start=(3,6) | Softmax\n",
    "print(\"Config 12: std_sarsa_windTrue_ss(3,6)_stratsoftmax\")\n",
    "df12, best12 = run_one_config_by_name('std_sarsa_windTrue_ss(3, 6)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90940a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 13: std_sarsa_windFalse_ss(0,4)_strateps_greedy\n",
      "Running single config: std_sarsa_windFalse_ss(0, 4)_strateps_greedy\n",
      "Saved: std_sarsa_windFalse_ss(0, 4)_strateps_greedy_results.csv std_sarsa_windFalse_ss(0, 4)_strateps_greedy_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.001, 'mean_eval_return': -6.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_sarsa_windFalse_ss(0, 4)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 13 – SARSA | wind=False | start=(0,4) | ε-greedy\n",
    "print(\"Config 13: std_sarsa_windFalse_ss(0,4)_strateps_greedy\")\n",
    "df13, best13 = run_one_config_by_name('std_sarsa_windFalse_ss(0, 4)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9146b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 14: std_sarsa_windFalse_ss(0,4)_stratsoftmax\n",
      "Running single config: std_sarsa_windFalse_ss(0, 4)_stratsoftmax\n",
      "Saved: std_sarsa_windFalse_ss(0, 4)_stratsoftmax_results.csv std_sarsa_windFalse_ss(0, 4)_stratsoftmax_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.01, 'mean_eval_return': -6.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[0, 4]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_sarsa_windFalse_ss(0, 4)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 14 – SARSA | wind=False | start=(0,4) | Softmax\n",
    "print(\"Config 14: std_sarsa_windFalse_ss(0,4)_stratsoftmax\")\n",
    "df14, best14 = run_one_config_by_name('std_sarsa_windFalse_ss(0, 4)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2fd8377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 15: std_sarsa_windFalse_ss(3,6)_strateps_greedy\n",
      "Running single config: std_sarsa_windFalse_ss(3, 6)_strateps_greedy\n",
      "Saved: std_sarsa_windFalse_ss(3, 6)_strateps_greedy_results.csv std_sarsa_windFalse_ss(3, 6)_strateps_greedy_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'eps_greedy', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.001, 'mean_eval_return': -1.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_sarsa_windFalse_ss(3, 6)_strateps_greedy'}\n"
     ]
    }
   ],
   "source": [
    "# Config 15 – SARSA | wind=False | start=(3,6) | ε-greedy\n",
    "print(\"Config 15: std_sarsa_windFalse_ss(3,6)_strateps_greedy\")\n",
    "df15, best15 = run_one_config_by_name('std_sarsa_windFalse_ss(3, 6)_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2de7c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 16: std_sarsa_windFalse_ss(3,6)_stratsoftmax\n",
      "Running single config: std_sarsa_windFalse_ss(3, 6)_stratsoftmax\n",
      "Saved: std_sarsa_windFalse_ss(3, 6)_stratsoftmax_results.csv std_sarsa_windFalse_ss(3, 6)_stratsoftmax_best.csv\n",
      "{'alg': 'sarsa', 'strategy': 'softmax', 'alpha': 0.1, 'gamma': 0.7, 'param': 0.01, 'mean_eval_return': -1.0, 'std_eval_return': 0.0, 'cfg_start_state': array([[3, 6]]), 'cfg_transition_prob': 1.0, 'cfg_wind': False, 'config_name': 'std_sarsa_windFalse_ss(3, 6)_stratsoftmax'}\n"
     ]
    }
   ],
   "source": [
    "# Config 16 – SARSA | wind=False | start=(3,6) | Softmax\n",
    "print(\"Config 16: std_sarsa_windFalse_ss(3,6)_stratsoftmax\")\n",
    "df16, best16 = run_one_config_by_name('std_sarsa_windFalse_ss(3, 6)_stratsoftmax', episodes=600, eval_episodes=20)\n",
    "print(best16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 17 – Four-Room | Q-Learning | goalChange=True\n",
    "print(\"Config 17: four_q_goalchangeTrue_strateps_greedy \")\n",
    "df17, best17 = run_one_config_by_name('four_q_goalchangeTrue_strateps_greedy', episodes=600, eval_episodes=20)\n",
    "print(best17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 18 – Four-Room | Q-Learning | goalChange=False\n",
    "print(\"\\n===== Config 18: four_q_goalchangeFalse_strateps_greedy =====\")\n",
    "df18, best18 = run_one_config_by_name('four_q_goalchangeFalse_strateps_greedy', episodes=600, eval_episodes=10)\n",
    "print(best18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e387a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f9a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
